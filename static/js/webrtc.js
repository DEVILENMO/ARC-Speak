// å…¨å±€å˜é‡
let localStream = null;
let peerConnections = {};
const configuration = { 
    iceServers: [
        { urls: 'stun:stun.l.google.com:19302' },
        { urls: 'stun:stun1.l.google.com:19302' }
    ] 
};
let isMuted = false;
let webrtcSocket = null; // ç”¨äºå­˜å‚¨Socket.IOå®ä¾‹
let webrtcCurrentUserId = null; // ç”¨äºå­˜å‚¨å½“å‰ç”¨æˆ·ID
let currentChannelIdForWebRTC = null;
const DEBUG_LOOPBACK = false; // <--- æ·»åŠ è¿™ä¸ªå¼€å…³ï¼Œè®¾ä¸º true æ¥å¯ç”¨æœ¬åœ°å›ç¯

// åˆå§‹åŒ–WebRTCæ¨¡å—ï¼Œä¼ å…¥socketå®ä¾‹å’Œå½“å‰ç”¨æˆ·ID
function initializeWebRTC(socketInstance, currentUserId) {
    webrtcSocket = socketInstance;
    webrtcCurrentUserId = currentUserId;

    // å°†æ‰€æœ‰ä¾èµ– socket çš„äº‹ä»¶ç›‘å¬å™¨ç§»åˆ°è¿™é‡Œ
    webrtcSocket.on('voice_signal', handleVoiceSignal);
    webrtcSocket.on('user_speaking', handleUserSpeaking);
    webrtcSocket.on('user_mute_status', handleUserMuteStatus);
    console.log("WebRTC signaling initialized with socket and user ID.");
}

// åˆå§‹åŒ–åª’ä½“æµ
async function initializeMedia() {
    try {
        const preferredMicrophone = localStorage.getItem('selectedAudioInput'); // <--- ç¡®ä¿è¿™é‡Œç”¨çš„æ˜¯ selectedAudioInput
        const constraints = {
            audio: preferredMicrophone && preferredMicrophone !== 'default' ? 
                { deviceId: { exact: preferredMicrophone } } : 
                true
        };
        
        localStream = await navigator.mediaDevices.getUserMedia(constraints);
        console.log('éº¦å…‹é£å·²åˆå§‹åŒ–', localStream);

        if (DEBUG_LOOPBACK && localStream) {
            const loopbackAudio = document.createElement('audio');
            loopbackAudio.autoplay = true;
            loopbackAudio.muted = true; // å¼€å§‹æ—¶é™éŸ³ï¼Œé˜²æ­¢å•¸å«ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡æµè§ˆå™¨æ§åˆ¶æ’­æ”¾
            loopbackAudio.srcObject = localStream;
            // å¯ä»¥è€ƒè™‘å°†è¿™ä¸ªå…ƒç´ æ·»åŠ åˆ°DOMä¸­å¹¶éšè—ï¼Œæˆ–è€…ä¸æ·»åŠ ï¼Œç›´æ¥æ’­æ”¾
            // document.getElementById('remote-audio-container').appendChild(loopbackAudio); 
            // console.log("æœ¬åœ°å›ç¯å·²å¯ç”¨ã€‚ç¡®ä¿è§£é™¤é™éŸ³ä»¥å¬åˆ°è‡ªå·±å£°éŸ³ã€‚");
            // å®é™…ä¸Šï¼Œä¸ºäº†ç›´æ¥å¬åˆ°ï¼Œæˆ‘ä»¬å¯ä»¥ä¸è®¾ç½® muted = trueï¼Œä½†è¦å°å¿ƒå¯èƒ½çš„å•¸å«
            // å¦‚æœç›´æ¥æ’­æ”¾ï¼Œåˆ™éœ€è¦ç¡®ä¿è¾“å‡ºè®¾å¤‡å’Œè¾“å…¥è®¾å¤‡ä¸åŒï¼Œæˆ–è€…ç”¨æˆ·æˆ´è€³æœº
            // æ›´å®‰å…¨çš„æ–¹å¼æ˜¯åˆ›å»ºä¸€ä¸ªæ–°çš„AudioContextæ¥å¤„ç†å›ç¯å¹¶æ§åˆ¶éŸ³é‡

            // å®‰å…¨çš„å›ç¯æ–¹å¼ï¼Œé¿å…ç›´æ¥æ’­æ”¾å¯èƒ½å¯¼è‡´çš„å•¸å«
            const audioContext = new (window.AudioContext || window.webkitAudioContext)();
            const source = audioContext.createMediaStreamSource(localStream);
            const gainNode = audioContext.createGain();
            gainNode.gain.value = 0.7; // è®¾ç½®ä¸€ä¸ªé€‚ä¸­çš„å›ç¯éŸ³é‡ï¼Œé¿å…è¿‡å¤§
            source.connect(gainNode);
            gainNode.connect(audioContext.destination); // è¿æ¥åˆ°é»˜è®¤è¾“å‡º
            console.log("æœ¬åœ°è°ƒè¯•å›ç¯å·²å¯ç”¨ï¼ŒéŸ³é‡ 0.7");
        }

        // åœ¨æˆåŠŸè·å–localStreamåï¼Œè®¾ç½®éŸ³é‡æ£€æµ‹
        // (ç”± main.html ä¸­çš„ joinVoiceChannel è°ƒç”¨æ­¤å‡½æ•°åï¼Œå†è°ƒç”¨ setupVolumeDetection)

    } catch (error) {
        console.error('æ— æ³•è®¿é—®éº¦å…‹é£:', error);
        alert('æ— æ³•è®¿é—®éº¦å…‹é£ã€‚è¯·æ£€æŸ¥æµè§ˆå™¨æƒé™è®¾ç½®ã€‚');
    }
}

// åˆ›å»ºéŸ³é‡æ§åˆ¶èŠ‚ç‚¹
function createGainNode(stream, initialVolume) {
    const audioContext = new (window.AudioContext || window.webkitAudioContext)();
    const sourceNode = audioContext.createMediaStreamSource(stream);
    const gainNode = audioContext.createGain();
    gainNode.gain.value = initialVolume;
    sourceNode.connect(gainNode);
    
    const destinationNode = audioContext.createMediaStreamDestination();
    gainNode.connect(destinationNode);
    
    const newStream = destinationNode.stream;
    return { stream: newStream, gainNode };
}

// é™éŸ³æ§åˆ¶
function toggleMute() {
    if (!localStream || !webrtcSocket) return;
    
    isMuted = !isMuted;
    localStream.getAudioTracks().forEach(track => {
        track.enabled = !isMuted;
    });
    
    const muteBtn = document.getElementById('mute-btn');
    if (muteBtn) {
        muteBtn.textContent = isMuted ? 'å–æ¶ˆé™éŸ³' : 'é™éŸ³';
        muteBtn.style.backgroundColor = isMuted ? '#7289da' : '#f04747';
    }
    
    // ä½¿ç”¨ webrtcSocket å‘é€
    webrtcSocket.emit('update_mute_status', { is_muted: isMuted });
}

// åˆ›å»ºå¯¹ç­‰è¿æ¥
function createPeerConnection(userId) {
    if (peerConnections[userId]) return peerConnections[userId];
    
    // å…¼å®¹æ€§æ£€æŸ¥
    const RTCPeerConnection = window.RTCPeerConnection || window.mozRTCPeerConnection || window.webkitRTCPeerConnection;
    if (!RTCPeerConnection) {
        console.error("WebRTC PeerConnection is not supported by this browser.");
        alert("WebRTC PeerConnection is not supported by this browser.");
        return null;
    }

    const pc = new RTCPeerConnection(configuration);
    peerConnections[userId] = pc;
    
    if (localStream) {
        localStream.getTracks().forEach(track => pc.addTrack(track, localStream));
    }
    
    pc.onicecandidate = event => {
        if (event.candidate && webrtcSocket) {
            webrtcSocket.emit('voice_signal', {
                type: 'ice_candidate',
                candidate: event.candidate,
                recipient_id: userId
            });
        }
    };
    
    pc.onconnectionstatechange = event => {
        console.log(`è¿æ¥çŠ¶æ€ (${userId}):`, pc.connectionState);
        if (pc.connectionState === 'failed' || pc.connectionState === 'disconnected' || pc.connectionState === 'closed') {
            // å¯ä»¥è€ƒè™‘åœ¨è¿™é‡Œæ¸…ç† peerConnections[userId]
        }
    };
    
    pc.ontrack = event => {
        console.log('æ”¶åˆ°è¿œç¨‹æµ for user:', userId, event);
        const remoteAudioContainer = document.getElementById('remote-audio-container');
        if (!remoteAudioContainer) {
            console.error("Remote audio container not found!");
            return;
        }

        let remoteAudio = document.getElementById(`audio-${userId}`);
        if (!remoteAudio) {
            remoteAudio = document.createElement('audio');
            remoteAudio.id = `audio-${userId}`;
            remoteAudio.autoplay = true;
            
            const preferredSpeaker = localStorage.getItem('selectedAudioOutput'); // ä» settings.html è¯»å–é€‰æ‹©
            if (preferredSpeaker && preferredSpeaker !== 'default' && typeof remoteAudio.setSinkId === 'function') {
                remoteAudio.setSinkId(preferredSpeaker)
                    .then(() => console.log(`Audio output set to ${preferredSpeaker} for user ${userId}`))
                    .catch(error => console.error('æ‰¬å£°å™¨è®¾ç½®å¤±è´¥:', error));
            }
            remoteAudioContainer.appendChild(remoteAudio);
        }
        
        if (remoteAudio.srcObject !== event.streams[0]) {
            remoteAudio.srcObject = event.streams[0];
            remoteAudio.play().catch(e => console.error("Error playing remote audio:", e));
        }
    };
    
    return pc;
}

// åå•†è¿æ¥
async function negotiateConnection(userId) {
    if (!webrtcSocket) return;
    try {
        const pc = createPeerConnection(userId);
        if (!pc) return;
        
        const offer = await pc.createOffer();
        await pc.setLocalDescription(offer);
        
        webrtcSocket.emit('voice_signal', {
            type: 'offer',
            sdp: pc.localDescription,
            recipient_id: userId
        });
    } catch (error) {
        console.error(`è¿æ¥åå•†å¤±è´¥ (${userId}):`, error);
    }
}

// å¤„ç†ä¿¡ä»¤ (ä½œä¸ºå›è°ƒå‡½æ•°)
async function handleVoiceSignal(data) {
    if (!webrtcSocket || !data.sender_id) return;
    try {
        const pc = createPeerConnection(data.sender_id); // ç¡®ä¿å¯¹ç«¯è¿æ¥å·²åˆ›å»ºæˆ–è·å–
        if (!pc) return;

        if (data.type === 'offer') {
            await pc.setRemoteDescription(new RTCSessionDescription(data.sdp));
            const answer = await pc.createAnswer();
            await pc.setLocalDescription(answer);
            webrtcSocket.emit('voice_signal', {
                type: 'answer',
                sdp: pc.localDescription,
                recipient_id: data.sender_id
            });
        } else if (data.type === 'answer') {
            await pc.setRemoteDescription(new RTCSessionDescription(data.sdp));
        } else if (data.type === 'ice_candidate' && data.candidate) {
            try {
                await pc.addIceCandidate(new RTCIceCandidate(data.candidate));
            } catch (e) {
                console.warn('Failed to add ICE candidate immediately, possibly still connecting:', e.message);
                // ä¸å†ä½¿ç”¨setTimeoutç¼“å­˜ï¼Œä¾èµ–onconnectionstatechangeæˆ–å…¶ä»–æœºåˆ¶
            }
        }
    } catch (error) {
        console.error(`å¤„ç†ä¿¡ä»¤é”™è¯¯ (from ${data.sender_id}, type ${data.type}):`, error);
    }
}

// æ˜¾ç¤ºè¯­éŸ³æ´»åŠ¨ (æ›´æ–°ä»¥é€‚é…æ–°çš„å¡ç‰‡ç»“æ„å’Œç±»å)
function showSpeakingIndicator(userId, isSpeaking) {
    const userCard = document.getElementById(`voice-user-${userId}`);
    if (userCard) {
        if (isSpeaking) {
            userCard.classList.add('speaking');
        } else {
            userCard.classList.remove('speaking');
        }
    }
}

// éº¦å…‹é£éŸ³é‡æ£€æµ‹ (ä½¿ç”¨ app.py ä¸­å®šä¹‰çš„äº‹ä»¶å)
function setupVolumeDetection(channelId) {
    if (!localStream || !webrtcSocket || !webrtcCurrentUserId) return;
    currentChannelIdForWebRTC = channelId;
    
    const audioContext = new (window.AudioContext || window.webkitAudioContext)();
    const analyser = audioContext.createAnalyser();
    const microphone = audioContext.createMediaStreamSource(localStream);
    
    analyser.smoothingTimeConstant = 0.5; // å¢åŠ å¹³æ»‘æ—¶é—´ï¼Œå‡å°‘æ³¢åŠ¨
    analyser.fftSize = 256; // æ›´å°çš„fftSizeï¼Œå“åº”æ›´å¿«ï¼Œä½†ä¹Ÿå¯èƒ½æ›´å™ª
    const dataArray = new Uint8Array(analyser.frequencyBinCount);
    microphone.connect(analyser);

    let speakingCurrently = false;
    let silenceCounter = 0; // ç”¨äºæ£€æµ‹æŒç»­çš„é™é»˜
    const REQUIRED_SILENCE_FRAMES = 10; // éœ€è¦å¤šå°‘å¸§é™é»˜æ‰è®¤ä¸ºåœæ­¢è¯´è¯ (10 * ~16ms = ~160ms)
    const SPEAKING_AVG_THRESHOLD = 10; // æé«˜å¹³å‡éŸ³é‡é˜ˆå€¼
    const SPEAKING_PEAK_THRESHOLD = 30; // æ·»åŠ ä¸€ä¸ªå³°å€¼é˜ˆå€¼ï¼Œé˜²æ­¢å•ä¸ªçªæ³¢è¯¯åˆ¤

    function detectSpeaking() {
        if (!localStream || !analyser) { 
            return;
        }
        analyser.getByteFrequencyData(dataArray);
        let sum = 0;
        let peak = 0;
        for (const amplitude of dataArray) {
            sum += amplitude;
            if (amplitude > peak) {
                peak = amplitude;
            }
        }
        const average = sum / dataArray.length;
        
        // console.log(`Avg: ${average.toFixed(2)}, Peak: ${peak}`); // è°ƒè¯•æ—¶å–æ¶ˆæ³¨é‡Š

        if ((average > SPEAKING_AVG_THRESHOLD || peak > SPEAKING_PEAK_THRESHOLD) && !isMuted) {
            silenceCounter = 0; // é‡ç½®é™é»˜è®¡æ•°å™¨
            if (!speakingCurrently) {
                speakingCurrently = true;
                webrtcSocket.emit('user_speaking_status', { speaking: true, channel_id: currentChannelIdForWebRTC });
                showSpeakingIndicator(webrtcCurrentUserId, true);
                // console.log("User started speaking"); // è°ƒè¯•
            }
        } else {
            silenceCounter++;
            if (speakingCurrently && silenceCounter >= REQUIRED_SILENCE_FRAMES) {
                speakingCurrently = false;
                webrtcSocket.emit('user_speaking_status', { speaking: false, channel_id: currentChannelIdForWebRTC }); 
                showSpeakingIndicator(webrtcCurrentUserId, false);
                // console.log("User stopped speaking"); // è°ƒè¯•
            }
        }
        requestAnimationFrame(detectSpeaking);
    }
    requestAnimationFrame(detectSpeaking);
    console.log("Volume detection setup with new thresholds and logic.");
}

// å¤„ç†å…¶ä»–ç”¨æˆ·è¯´è¯çŠ¶æ€ (ä½¿ç”¨ app.py ä¸­å®šä¹‰çš„äº‹ä»¶å)
function handleUserSpeaking(data) {
    // data: { user_id: ..., speaking: ...}
    if (data.user_id !== webrtcCurrentUserId) { // åªå¤„ç†å…¶ä»–ç”¨æˆ·çš„çŠ¶æ€
        showSpeakingIndicator(data.user_id, data.speaking);
    }
}

// å¤„ç†ç”¨æˆ·é™éŸ³çŠ¶æ€ (ä¿æŒä¸å˜ï¼Œä½†ç¡®ä¿webrtcSocketå¯ç”¨)
function handleUserMuteStatus(data) {
    const userElement = document.getElementById(`voice-user-${data.user_id}`);
    if (userElement) {
        let muteIcon = userElement.querySelector('.mute-icon');
        if (!muteIcon && data.is_muted) {
            muteIcon = document.createElement('span');
            muteIcon.className = 'mute-icon';
            muteIcon.textContent = 'ğŸ”‡';
            muteIcon.style.marginLeft = '5px';
            userElement.appendChild(muteIcon);
        } else if (muteIcon && !data.is_muted) {
            muteIcon.remove();
        }
    }
}

// å½“é¡µé¢å…³é—­æ—¶ç¡®ä¿æ¸…ç†èµ„æº
window.addEventListener('beforeunload', function() {
    if (localStream) {
        localStream.getTracks().forEach(track => track.stop());
    }
    Object.keys(peerConnections).forEach(userId => {
        if (peerConnections[userId]) {
            peerConnections[userId].close();
        }
    });
    // inVoiceChannel å˜é‡åœ¨ webrtc.js ä¸­æœªå®šä¹‰ï¼Œè¿™ä¸ªé€»è¾‘å¯èƒ½éœ€è¦ç§»åˆ° main.html æˆ–é€šè¿‡å›è°ƒå¤„ç†
    // if (inVoiceChannel && webrtcSocket) { 
    //     webrtcSocket.emit('leave_voice_channel');
    // }
}); 

// æš´éœ²éœ€è¦ä»å¤–éƒ¨è°ƒç”¨çš„å‡½æ•° (å¦‚æœ webrtc.js è¢«å½“ä½œä¸€ä¸ªæ¨¡å—)
// export { initializeWebRTC, initializeMedia, toggleMute, createPeerConnection, negotiateConnection, setupVolumeDetection }; 